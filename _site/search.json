[
  {
    "objectID": "gymnasium.html",
    "href": "gymnasium.html",
    "title": "\nGymnasium\n",
    "section": "",
    "text": "Gymnasium\n\nJan 12, 2024\n\n\nEnv"
  },
  {
    "objectID": "rl.html",
    "href": "rl.html",
    "title": "\nCS 443\n",
    "section": "",
    "text": "CS 443\n\nNov 22, 2023\n\nAll notes are based on the course at UIUC CS 443 taught by professor  Nan Jiang (姜楠).\nAs a senior Computer Science student embarking on the CS 443 course on Reinforcement Learning, I am filled with a sense of purpose and enthusiasm. This course, guided by Professor Nam Jiang, represents not just a pivotal point in my academic journey but also an opportunity to create a valuable resource for future students who will navigate the complexities of this advanced field. My decision to take this course in the upcoming semester is driven by a fascination with the dynamic and impactful world of RL. I understand the challenges that lie ahead, having encountered rigorous coursework in my academic path, but my commitment to delving deeper into Reinforcement Learning is unwavering. I am motivated not only by my desire to expand my own understanding but also by the prospect of aiding others in comprehending and appreciating the intricacies of this cutting-edge area of computer science.\n\nMathematics\n\nSome notes on mahtematics see HERE! \n\nGymnasium\n\nSome notes on a RL environment see HERE! \n\n\nThe Plan\n\n\n\nDAgger\n\nMDP Notes\n\nIntroduction\n\nNotes\n\nMarkov Decision Process formulation\n\nMDP Notes\n\nValue function\n\nValue Notes\n\nBellman equation\n\nBellman equation Notes\n\nOptimality\n\noptimality Notes\n\nValue Iteration\n\nValue iteration Notes\n\n\n\n  Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain.  — Alan Turing\nYou insist that there is something a machine cannot do. If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that.  — John von Neumann"
  },
  {
    "objectID": "mdp.html",
    "href": "mdp.html",
    "title": "\nMarkov decision process\n",
    "section": "",
    "text": "Markov decision process\n\\[\n\\def\\emph#1{\\textit{#1}}\n\\]\nIn reinforcement learning, the interactions between the agent and the environment are often described by a state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), transition function \\(P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})\\); where \\(\\Delta(\\mathcal{S})\\) is the space of probability distributions over \\(\\mathcal{S}\\) (i.e., the probability simplex). \\(P(s^\\prime \\mid s, a)\\) is the probability of transitioning into state \\(s^\\prime\\) upton taking action \\(a\\) in state \\(s\\). A reward function \\(R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, R_{max}]\\). where \\(R_{max} &gt; 0\\) is a constant. \\(R(s, a)\\) is the immediate reward associated with taking action \\(a\\) in state \\(s\\). A discount factor \\(\\gamma \\in [0, 1)\\) , which defines the horizon for the problem."
  },
  {
    "objectID": "mdp.html#opengym",
    "href": "mdp.html#opengym",
    "title": "\nMarkov decision process\n",
    "section": "OpenGym",
    "text": "OpenGym\n\nInitialize the Environment\nimport gym \nenv = gym.make('MountainCar-v0')\n\n\nMountain car\nIn the Mountain Car Markov Decision Process (MDP), a car is randomly positioned at the lowest point of a sinusoidally-shaped valley. This MDP operates deterministically, providing a set of possible accelerative actions that can be executed to move the car either forward or backward. The objective is to judiciously use these accelerations to navigate the car to the target location, situated at the peak of the hill to the right. Within the gym framework, the mountain car scenario comes in two variants: one allowing for a discrete set of actions, and the other permitting a continuum of actions. The variant in question here is the one that employs discrete actions.\nclass MountainCarEnv(gym.Env):\n    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.5\n        self.goal_velocity = goal_velocity\n\n        self.force = 0.001\n        self.gravity = 0.0025\n\n        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n\n        self.render_mode = render_mode\n\n        self.screen_width = 600\n        self.screen_height = 400\n        self.screen = None\n        self.clock = None\n        self.isopen = True\n\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)"
  },
  {
    "objectID": "algorithms.html",
    "href": "algorithms.html",
    "title": "\nCS 374\n",
    "section": "",
    "text": "CS 374\nAs a senior Computer Science student creating this resource, I felt a deep sense of responsibility not just towards my own academic growth but also for the benefit of future students grappling with the complexities of the renowned CS 374 algorithms course. My journey through this course, taught by Professor Erickson during the Fall 2023 semester, was challenging. Despite struggling significantly with the material, I was driven by a desire to immerse myself more deeply in the subjects taught.\nThis endeavor is not just an exercise in comprehension but a step towards mastery. By diving into and clearly explaining the course content, I’m hoping to deepen my understanding and get more comfortable with these complex ideas. My hope is that this process will not only aid my own progression in computer science but also serve as a valuable guide for others on a similar path. Please see  Jeff’s website and  Jeff’s book as his book is freely available online.\nSome notes on data structures HERE!"
  },
  {
    "objectID": "algorithms.html#coursework",
    "href": "algorithms.html#coursework",
    "title": "\nCS 374\n",
    "section": "Coursework",
    "text": "Coursework\n\nSection 1\n\nString induction\n\nLab1a: String induction\n\nLanguages and regular expressions\n\nLab1b: Regular expressions\n\nDFAs: intuition, definition, examples\n\nLab2a: DFAs\n\nDFAs: product construction, closure, automatic=regular\n\nLab2b: DFA product construction\n\nProving nonregularity via fooling sets; NFAs: intuition and examples \n\nLab3a: Proving nonregularity\n\nNFAs; ε-transitions, equivalence with DFAs\n\nLab3b: Regular expression to NFA to DFA (to regular expression)\n\nLanguage transformations\n\nLab4a: Language transformations\n\nContext-free languages and grammars \n\nLab4b: Context-free languages and grammars\n\nTuring machines \n\nLab5a: More language transformations\n\n\n\n\nSection 2\n\nRecursion: Hanoi, mergersort, quicksort\n\nLab6a: Binary search\n\nDevide and conquer: selection, multiplication\n\nLab6b: Fun with Karatsuba\n\nBactracking: n queens, game trees, text segmentation\n\nLab7a: Backtracking\n\nDynamic programming: Fibonacci, text segmentation again\n\nLab7b: Dynamic programming\n\nSequence dynamic programming: Edit distance \n\nLab8a: More dynamic programming\n\nTree-shaped dynamic programming: Carpentry\n\nLab8b: Return of the son of revenge of dynamic programming\n\nGraphs: definitions, representations, data structures, traversal\n\nLab9a: Graph modeling\n\nDepth-first search, topological sort \n\nLab9b: Topological sort\n\nDAG DP, strong components; generic shortest paths, BFS, DFS, and Dijkstra \n\nLab10a: Shortest paths\n\nShortest paths via Dijkstra and Bellman-Ford \n\nLab10b: All-pairs shortest paths\n\nBellman-Ford again and Floyd-Warshall \n\nLab11a: Solve it both ways\n\n\n\n\nSection 3\n\nReductions: Cliques and friends, Hamiltoninan cycles\n\nLab12a: Reductions\n\nP vs NP, NP-hardness, 3SAT, reduction to max independent set\n\nLab12b: NP-hardness proofs\n\nNP-harness: Vertex cover to Hamiltoninan cycle\n\nLab13a: More NP-hardness proofs\n\nNP-harness: Why bother, Choosing which problem to reduce from\n\nLab13b: Even more NP-hardness proofs\n\nUndecidability: code is data, the halting problem \n\nLab14a: Yet even still more NP-hardness practice\n\nUndecidability: reductions and Rice’s theorem\n\nLab14b: Using Rice’s Theorem\nLab14c: Undecidability Reductions\n\n\n\n  I propose to consider the question, “Can machines think?”  — Alan Turing, “Computing Machinery and Intelligence” (1950)\nIf you find that you’re spending almost all your time on theory, start turning some attention to practical things; it will improve your theories. If you find that you’re spending almost all your time on practice, start turning some attention to theoretical things; it will improve your practice.  — Donald Knuth\nPremature optimization is the root of all evil.  — Donald Knuth, “Structured Programming with Go To Statements” (1974)\nYoung man, in mathematics you don’t understand things. You just get used to them.  — John von Neumannxw\nDealing with failure is easy: Work hard to improve. Success is also easy to handle: You’ve solved the wrong problem. Work hard to improve.  — Alan Perlis, “Epigrams on Programming” (1982)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "\nAbout\n",
    "section": "",
    "text": "About\n\n\nSee my  website \nThis blog was largerly motivated by  Andrej Karpathy  and a personal professor at the University of Illinois at Urbana-Champaign  Jeff Erickson."
  },
  {
    "objectID": "lab7b.html",
    "href": "lab7b.html",
    "title": "\nDymamic programming\n",
    "section": "",
    "text": "Dymamic programming\nNov 22, 2023\nHow long is the longest increasing subsequence?\n4 1 2 7 6 5 8 9 3   1 2 6 8 9\nLets define a couple defintions to fromulate this:"
  },
  {
    "objectID": "lab7b.html#visulization-of-sequence",
    "href": "lab7b.html#visulization-of-sequence",
    "title": "\nDymamic programming\n",
    "section": "Visulization of sequence",
    "text": "Visulization of sequence\n\n\nCode\nlibrary(ggplot2)\n\n# Your original sequence\nsequence &lt;- c(4, 1, 2, 7, 6, 5, 8, 9, 3)\n\n# The known LIS\nlis &lt;- c(1, 2, 7, 8, 9)\n\n# Create a data frame for the original sequence\ndata &lt;- data.frame(x = 1:length(sequence), y = sequence)\n\n# Identify the indices of the LIS in the original sequence\nlis_indices &lt;- match(lis, sequence)\n\n# Create a data frame for the LIS\nlis_data &lt;- data.frame(x = lis_indices, y = lis)\n\n# Plotting the sequence and the LIS\nggplot(data, aes(x, y)) + \n  geom_point() + \n  geom_line(color = \"gray\") +\n  geom_point(data = lis_data, aes(x, y), color = \"red\") +\n  geom_line(data = lis_data, aes(x, y), color = \"red\") +\n  scale_x_continuous(breaks = 1:length(sequence)) +  \n  scale_y_continuous(breaks = 1:max(sequence)) +  \n  labs(x = \"Index\", y = \"Value\", title = \"Sequence with Highlighted LIS 1 2 7 8 9\")\n\n\n\n\n\n\n\n\nFigure 1: Plot of the sequence with the highlighted LIS 1 2 7 8 9\n\n\n\n\n\nThere could be multiple subsequences but we only care about the maximum increasing subsequence.\n\nHere is my animated plot:\n\n\n\nAnimated Plot\n\n\n\nDymamic programming\n\n\nDescribe and analyze dynamic programming algorithms for the following longest-subsequence problems.\n\nGiven an array \\(A[1...n]\\) of integers, compute the length of a longest increasing subsequence of \\(A\\)."
  },
  {
    "objectID": "ramsey3.html",
    "href": "ramsey3.html",
    "title": "\nRamsey Theory\n",
    "section": "",
    "text": "Ramsey Theory\n\nJan 13, 2024\n\nMy interest in Ramsey Theory, a fascinating field that explores the emergence of patterns within sufficiently large systems, was sparked by my exploration of topics in graph theory. I was looking to learn about order from choas I started with the example of the pigeonhole principle\n\nTheorem (Pigeonhole Princicple)\n\nIf there exists \\(m\\) pigeonoles containing \\(n\\) pigeons, where \\(n &gt; m\\), then at least one of the pigeonholes must contain at least 2 pigeons."
  },
  {
    "objectID": "lab1a.html",
    "href": "lab1a.html",
    "title": "\nString Induction\n",
    "section": "",
    "text": "String Induction\n\nNov 22, 2023"
  },
  {
    "objectID": "DavidMarr.html",
    "href": "DavidMarr.html",
    "title": "\nDMT\n",
    "section": "",
    "text": "DMT\n\n\nHow do we build intelligent machines?\n\n\nComing Soon!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jorge Velez",
    "section": "",
    "text": "I drink a lot of coffee and I try to be a better computer scientist over time. ☕ 🌎 🌌 This blog serves as an information space focusing on areas that interest me and that I would like to research..\n\nSorry for any delays, broken links, and ugly layouts; still working on finishing this up."
  },
  {
    "objectID": "index.html#hello-world",
    "href": "index.html#hello-world",
    "title": "Jorge Velez",
    "section": "",
    "text": "I drink a lot of coffee and I try to be a better computer scientist over time. ☕ 🌎 🌌 This blog serves as an information space focusing on areas that interest me and that I would like to research..\n\nSorry for any delays, broken links, and ugly layouts; still working on finishing this up."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jorge Velez",
    "section": "Education",
    "text": "Education\n University of Illinois at Urbana-Champaign | Bachelros of Science in Computer Science | Aug 2022 - Dec 2024\n Atttend my local community college to study computer science and mathematics | Agust 2021 - May 2022"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jorge Velez",
    "section": "Experience",
    "text": "Experience\n Argonne National Lab| Research | May 2022 - August 2022"
  },
  {
    "objectID": "cs.html",
    "href": "cs.html",
    "title": "\nComputer Science\n",
    "section": "",
    "text": "Computer Science\n\n\nJan 16, 2024 Statistics and Probability 2 Notes dedicated to Stat 410\nJan 15, 2024 Database Systems Notes dedicated to CS 411\nJan 12, 2024 Ramsey Theory Order From Chaos\nNov 22, 2023 Progrmg Languages & Compilers Notes dedicated to CS 421\nDec 31, 2023 Neural Networks Initially, neural networks were met with skepticism. However, a dedicated few persevered, pushing the boundaries of artificial intelligence. Their relentless pursuit and innovation turned a once-doubtful idea into a groundbreaking reality. Today, their work stands as a testament to human ingenuity, having significantly transformed technology and our understanding of machine learning.\nDec 31, 2023 The story of out time Llama2: Open Foundation and Fine-Tuned Chat Models.” This post offers a comprehensive and engaging analysis of LLMs, starting from their foundational concepts to their complex mechanisms of learning from extensive datasets to mimic human language. We delve into the intricacies of the Llama2 model, discussing its innovative approaches in open foundation training and the nuances of its fine-tuning for chat applications. Our journey through this post not only highlights the transformative impact of LLMs across various sectors but also addresses the ethical dilemmas and potential future advancements in this field.\nDec 31, 2023 P vs NP computational complexity in the heavens The P vs NP problem, often regarded as the most significant unresolved issue in theoretical computer science, is one of the seven Millennium Prize Problems identified by the Clay Mathematics Institute, offering a reward of one million dollars for a conclusive proof or disproof. In simple terms, ‘P’ represents a category of problems that are comparatively easy to solve, whereas ‘NP’ encompasses problems that are, on the surface, extremely challenging. If P were equal to NP, it would suggest that these seemingly difficult problems actually have straightforward solutions. However, the intricacies of this concept are more complex.\n\nFinance"
  },
  {
    "objectID": "cs421.html",
    "href": "cs421.html",
    "title": "\nCS 421\n",
    "section": "",
    "text": "CS 421\n\n\n\nBasic OCaml Programming"
  },
  {
    "objectID": "lab8a.html",
    "href": "lab8a.html",
    "title": "\nEdit distance\n",
    "section": "",
    "text": "Edit distance\n\n\n\nThe minimum number of character insertions, deletions, and substitutions needed to convert one string into another is known as the edit distance between those two strings.\nGiven two strings word1 and word2, return the minimum number of operations required to convert word1 to word2.\nYou have the following three operations permitted on a word:\n\nInsert a character\nDelete a character\nReplace a character"
  },
  {
    "objectID": "dataStructures.html",
    "href": "dataStructures.html",
    "title": "\nData Strcutures\n",
    "section": "",
    "text": "Data Strcutures\n\nDec 31, 2023\n\n\nLinked List: notes\nHash Table: notes\nBinary Tress\nRandom Binary Search Trees\nRed-Black Trees\nHeaps\nSorting algorithms: notes\nGraph: notes\nExternal Memory Searching"
  },
  {
    "objectID": "linearAlgebra.html",
    "href": "linearAlgebra.html",
    "title": "\nLinear Algebra\n",
    "section": "",
    "text": "Linear Algebra\n\nJan 15, 2024\n\n Vector spaces\nA vector space \\(V\\) is a set, the elements of which are called \\(\\textbf{vectors}\\), on which two operations are defined: vectors can be added together, and vectors can be multiploed by real numbers (\\(\\textbf{scalars}\\)).\nWith this \\(V\\) must statisfy the following:\n\nThere exists an additive identity \\(\\mathbf{0}\\) in \\(V\\) such that \\(\\mathbf{x} + \\mathbf{0} = \\mathbf{x}\\) for all \\(\\mathbf{x} \\in V\\)\nFor each \\(\\mathbf{x} \\in V\\), there exists and additive inverse, \\(-\\mathbf{x}\\), such that \\(\\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}\\)\nThere exists a multiplicative identity (written 1) in \\(\\mathbb{R}\\) such that \\(1\\mathbf{x} = \\mathbf{x}\\) for all \\(\\mathbf{x} \\in V\\)\nCommutativity: \\(\\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y} \\in V\\)\nAssociativity: \\((\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z})\\) and \\(\\alpha(\\beta \\mathbf{x}) = (\\alpha\\beta)\\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in V\\) and \\(\\alpha, \\beta \\in \\mathbb{R}\\)\nDistributivity: \\(\\alpha(\\mathbf{x} + \\mathbf{y}) = \\alpha\\mathbf{x} + \\alpha\\mathbf{y}\\) and \\((\\alpha + \\beta)\\mathbf{x} = \\alpha\\mathbf{x} + \\beta\\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y} \\in V\\) and \\(\\alpha, \\beta \\in \\mathbb{R}\\)\n\nA set of vectors \\(\\mathbf{v}_1, \\cdots, \\mathbf{v}_n \\in V\\) is said to be \\(\\textbf{linearly independent}\\) if\n\\[\n    \\alpha \\mathbf{v}_n + \\cdots + \\alpha_n\\mathbf{v}_n = \\mathbf{0} \\quad \\text{ imlpies } \\quad\\alpha_1 = \\cdots = \\alpha_n = 0.\n\\]\nThe \\(\\textbf{span}\\) of \\(\\mathbf{v}_1, \\cdots, \\mathbf{v}_n \\in V\\) is the set of all vectors that can be expressed of a linear combination of them:\n\\[\n    \\text{span}\\{\\mathbf{v}_1, \\cdots, \\mathbf{v}_n\\} = \\{\\mathbf{v} \\in V : \\exists\\alpha, \\cdots, \\alpha_n \\text{ such that } \\alpha\\mathbf{v}_1 + \\cdots + \\alpha_n\\mathbf{v}_n = \\mathbf{v}\\}\n\\]\nIf the set of vectors is linearly independent and its span is the whole \\(V\\), those vectors are said to be a \\(\\textbf{basis}\\) for \\(V\\). In fact, every linear independent set of vectors forms a basis for its span.\nIf a vector space is spanned by a finite numbers of vectors, it is said to be \\(\\textbf{finite-dimensional}\\). Otherwise it is \\(\\textbf{infinite-dimensional}\\). The number of vectors in a basis for a finite-dimensional vector space \\(V\\) is called the \\(\\textbf{dimension}\\) of \\(V\\) and demoted dim \\(V\\).\n Euclidean space\nThe quintessential vector space is \\(\\textbf{Eculidean space}\\), which we denote \\(\\mathbb{R}^n\\). The vectors in this space consist of \\(n\\)-tuples of real numbers:\n\\[\n    \\mathbf{x} = \\left(x_1, x_2, \\cdots, x_n\\right)\n\\]\nIt is also useful to consider them as a \\(n \\times 1\\) matrices, or \\(\\textbf{column vectors}\\):\n\\[\n    \\mathbf{x} = \\begin{bmatrix}\n        x_{1} \\\\\n        x_{1} \\\\\n        \\vdots \\\\\n        x_{n}\n        \\end{bmatrix}\n\\]\nAddition and scalar multiplication are defined component-wise on vectors in \\(\\mathbb{R}^n\\). \\[\n    \\mathbf{x} + \\mathbf{y} = \\begin{bmatrix}\n        x_{1} + y{1} \\\\\n        x_{1} + y_{2}\\\\\n        \\vdots \\\\\n        x_{n} + y_{n}\n        \\end{bmatrix}, \\qquad \\alpha\\mathcal{x} = \\begin{bmatrix}\n                                                    \\alpha x_{1} \\\\\n                                                    \\vdots \\\\\n                                                    \\alpha x_{n}\n                                                    \\end{bmatrix}\n\\]\nEuclidean space is used to mathematically represent physical space, with notions such as distance, length, and angles. Although it becomes hard to visualize fot \\(n &gt; 3\\), these concepts generalize mathematically in obvious ways. Even when you’re wokring in more general settings than \\(\\mathbb{R}^n\\), it is often useful to visualize vector addition and scalalr multiplication in terms of \\(2D\\) vectors in the plane or \\(3D\\) vectors in space.\nSubspaces\nVector spaces can contain other vector spaces. If \\(V\\) is a vector space, then \\(\\mathcal{S} \\subseteq V\\) is said to be a \\(\\textbf{subspace}\\) of \\(V\\) if:\n\n\\(\\mathbf{0} \\in \\mathcal{S}\\)\n\\(\\mathcal{S}\\) is closed under addition: \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}\\) implies \\(\\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}\\)\n\\(\\mathcal{S}\\) is closed under scalar multiplication: \\(\\mathbf{x} \\in \\mathcal{S}, \\alpha \\in \\mathbb{R}\\) implies \\(\\alpha\\mathbf{x} \\in \\mathcal{S}\\)\n\nNote that \\(V\\) is always a subsapce of \\(V\\), as is the trivial vector space which contains only \\(\\mathbf{0}\\). As a concrete example, a line passing through the origin is a subsapce of Eculidean space. If \\(U\\) and \\(W\\) are subspaces of \\(V\\), then their sum is defined as\n\\[\n    U + W = \\{\\mathbf{u} + \\mathbf{w} \\mid \\mathbf{u} \\in U, \\mathbf{w} \\in W \\}\n\\]\nIf \\(U \\cap W = \\{\\mathbf{0}\\}\\), the sum is said to be a \\(\\textbf{direct sum}\\) and written \\(U \\oplus W\\). Every vector in \\(U \\oplus W\\) can be written uniquely as \\(\\mathbf{u} + \\mathbf{w}\\) for some \\(\\mathbf{u} \\in U\\) and \\(\\mathbf{w} \\in W\\).\nThe dimensions of sums of subspaces obey a friendly relationship:\n\\[\n    \\text{dim}(U + W) = \\text{dim } U + \\text{dim } W - \\text{dim}(U \\cap W)  \n\\]\nIt follows that\n\\[\n    \\text{dim}(U \\oplus W) = \\text{dim } U + \\text{dim } W\n\\]\nsince \\(\\text{dim}(U \\cap W) = \\text{dim}(\\{\\mathbf{0}\\}) = 0\\) if the sum is direct.\nLinear maps\n Matrices and transposes\n\n\\(A\\) is a \\(m \\times n\\) real matrix, wrtitten \\(A \\in \\mathbb{R}^{m \\times n}\\), if: \\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\n\nwhere \\(a_{i, j} \\in \\mathbb{R}\\). The \\((i, j)\\)th entry of \\(A\\) is \\(A_{i, j} = a_{i, j}\\).\n\nThe transpose of \\(A \\in \\mathbb{R}\\) is define as: \\[\nA^{T} = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\n\nsuch that, \\((A^{T})_{i, j} = A_{j, i}\\)\nNote: \\(x \\in \\mathbb{R}^{n}\\) is considered to be a columnn vector in \\(\\mathbb{R}^{n \\times 1}\\)\nSums and products of matrcies \n\nThe sum of matrices \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(B \\in \\mathbb{R}^{m \\times n}\\) is the matrix \\(A + B \\in \\mathbb{R}^{m \\times n}\\) such that \\[\n  (A + B)_{i, j} = A_{i, j} + B_{i, j}\n\\]\nThe product of matrices \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(B \\in \\mathbb{R}^{n \\times \\ell}\\) is the matrix \\(AB \\in \\mathbb{R}^{m \\times \\ell}\\) such that\n\n\\[\n    (AB)_{i, j} = \\sum_{k=1}^n   A_{i, k} B_{k, j}\n\\]"
  },
  {
    "objectID": "mathematics.html",
    "href": "mathematics.html",
    "title": "\nMathematics\n",
    "section": "",
    "text": "Mathematics\n\nJan 15, 2024\n\n\nLinear Algebra: notes\nProbability: notes"
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "\nProbability\n",
    "section": "",
    "text": "Probability\n\nJan 15, 2024\n\nProbability theory helps us deal with modeling with uncertainty.\nSuppose we perform a experiment like tossing a coin which has a fixed set of possible outcomes. This set is called the \\(\\textbf{sample sapce}\\) and we denote this space with \\(\\Omega\\).\nWe would like to define probabilities for some \\(\\textbf{events}\\), which are subsets of \\(\\Omega\\). The set of events is denoted \\(\\mathcal{F}\\). The \\(\\textbf{complement}\\) of the event \\(\\mathcal{A}\\) is another event, \\(\\mathcal{A}^{c} = \\Omega \\setminus \\mathcal{A}\\)\nThen we can define a \\(\\textbf{probability measure} \\:\\: \\mathbb{P}: \\mathcal{F} \\rightarrow [0, 1]\\) which must satisfy\n\n\\(\\mathbb{P}(\\Omega) = 1\\)\n\\(\\textbf{Countable addivity:}\\) for any countable collection of disjoint sets \\(\\{\\mathcal{A}_i\\} \\subseteq \\mathcal{F}\\),\n\n\\[\n    \\mathbb{P}\\left(\\bigcup_i \\mathcal{A}_i\\right) = \\sum_i \\mathbb{P}(\\mathcal{A}_i)\n\\]\nThe triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is called a \\(\\textbf{probability space}\\).\nIf \\(\\mathbb{P}(\\mathcal{A}) = 1\\), we say that \\(\\mathcal{A}\\) occurs \\(\\textbf{almost surely}\\), and conversely \\(\\mathcal{A}\\) occurs \\(\\textbf{almost never}\\) if \\(\\mathbb{P}(\\mathcal{A}) = 0.\\)\n\\(\\textbf{Proposition}\\): Let \\(\\mathcal{A}\\) be an event. Then\n\n\\(\\mathbb{P}(\\mathcal{A^c}) = 1 - \\mathbb{P}(\\mathcal{A})\\)\nIf \\(\\mathcal{B}\\) is an event and \\(\\mathcal{B} \\subseteq \\mathcal{A}\\), then \\(\\mathbb{P}(\\mathcal{B}) \\leq \\mathbb{P}(\\mathcal{A})\\).\n\\(0 = \\mathbb{P}(\\varnothing) \\leq \\mathbb{P}(\\mathcal{A}) \\leq \\mathbb{P}(\\Omega) = 1\\)\n\n\\(Proof\\):\nUsing the countable additivity of \\(\\mathbb{P}\\), we have\n\\[\n    \\mathbb{P}(\\mathcal{A}) + \\mathbb{P}(\\mathcal{A^c}) = \\mathbb{P}(\\mathcal{A} \\cup \\mathcal{A}^c) = \\mathbb{P}(\\Omega) = 1\n\\]\nTo show 2. suppose \\(\\mathcal{B} \\in \\mathcal{F}\\) and \\(\\mathcal{B} \\subseteq \\mathcal{A}\\). Then\n\\[\n    \\mathbb{P}(\\mathcal{A}) = \\mathbb{P}(\\mathcal{B} \\cup (\\mathcal{A} \\setminus \\mathcal{B})) = \\mathbb{P}(\\mathcal{B}) + \\mathbb{P}(\\mathcal{A} \\setminus \\mathcal{B}) \\geq \\mathbb{P}(\\mathcal{B})\n\\]\nas claimed.\nDiscrete random variables\n\nA random variable denoted as \\(\\text{r.v}\\) is a quantity that probabilistically takes on any of a possible range of values.\nA random variable \\(X\\) is descrete if it takes values in a countable set \\(\\mathcal{X} = \\{x_1, x_2, \\cdots,\\}.\\)\nMost random varibales have some certain distributioin for example Bernoulli, Binomial, Poisson, Geometric."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\nIntroduction\n",
    "section": "",
    "text": "Introduction\n\n\nShortest Path\nFinding the shortest path in this graph:\n\nBased on actions taken by the agent"
  },
  {
    "objectID": "probStats/statRev.html",
    "href": "probStats/statRev.html",
    "title": "\nStatistics and Probability review\n",
    "section": "",
    "text": "Statistics and Probability review\n\n\nRandom variables\n\n\n\n\n\n\n\nDiscrete (Probability Mass Function, p.m.f)\nContinuous (Probability Density Function, p.d.f)\n\n\n\n\n\\(p(x) = \\mathbb{P}(X = x)\\)\n\\(f(x)\\)\n\n\n\\(\\forall x \\quad 0 \\leq p(x) \\leq 1\\)\n\\(\\forall x \\quad f(x) \\geq 0\\)\n\n\n\\(\\sum_{\\text{all } x} p(x) = 1\\)\n\\(\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\)\n\n\n\nCumulative distributioin function\n\\(\\text{c.d.f }\\)\n\\(F(x) = \\mathbb{P}(X \\leq x)\\)\n\n\n\n\n\n\n\nDiscrete (c.d.f)\nContinuous (c.d.f)\n\n\n\n\n\\(F(x) = \\sum_{y \\leq x} p(y)\\)\n\\(F(x) = \\int_{-\\infty}^{x} f(y) \\, dy\\)\n\n\n\nExpected value\n\\(\\mathbb{E}(X) = \\mathbf{\\mu}_X\\)\n\n\n\n\n\n\n\nDiscrete Expected value\nContinuous Expected value\n\n\n\n\n\\(\\text{If }\\underset{\\text{all } x} \\sum | x | \\cdot p(x) &lt; \\infty\\)\n\\(\\int_{-\\infty}^{\\infty} | x | \\cdot f(x)dx &lt; \\infty\\)\n\n\n\\(\\mathbb{E}(X) = \\underset{\\text{all } x} \\sum x \\cdot p(x)\\)\n\\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x\\cdot f(x)dx\\)\n\n\n\\(\\text{If }\\underset{\\text{all } x} \\sum | g(x) | \\cdot p(x) &lt; \\infty\\)\n\\(\\int_{-\\infty}^{\\infty} | g(x) | \\cdot f(x)dx &lt; \\infty\\)\n\n\n\\(\\mathbb{E}(g(X)) = \\underset{\\text{all } x} \\sum g(x) \\cdot p(x)\\)\n\\(\\mathbb{E}(g(X)) = \\int_{-\\infty}^{\\infty} g(x) \\cdot f(x)dx\\)\n\n\n\nVariance\n\\(\\text{Var}(X) = \\sigma^2_X = \\mathbb{E}(\\left[ X - \\mu_{X} \\right]^2) = \\mathbb{E}(X^2) - \\left[\\mathbb{E}(X)\\right]^2\\)\n\n\n\n\n\n\n\nDiscrete Variance\nContinuous Variance\n\n\n\n\n\\(\\text{Var}(X) = \\underset{\\text{all } x} \\sum (x - \\mu_X)^2 \\cdot p(x)\\)\n\\(\\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - \\mu_X)^2 \\cdot f(x)dx\\)\n\n\n\\(= \\underset{\\text{all } x} \\sum x^2 \\cdot p(x) - \\left[\\mathbb{E}(X)\\right]^2\\)\n\\(= \\left[\\int_{-\\infty}^{\\infty} x^2 \\cdot f(x)dx\\right] - \\left[\\mathbb{E}(X)\\right]^2\\)\n\n\n\nMoment-generating function\n\\(M_X(t) = \\mathbb{E}(\\mathcal{e}^{tX})\\)\n\n\n\n\n\n\n\nDiscrete moment-generating function\nContinuous moment-generating function\n\n\n\n\n\\(M_X(t) = \\underset{\\text{all } x} \\sum \\mathcal{e}^{tx} \\cdot p(x)\\)\n\\(M_X(t) = \\int_{-\\infty}^{\\infty}  \\mathcal{e}^{tx} \\cdot f(x)dx\\)\n\n\n\nExample 1\n\n\n\n\\(x\\)\n\\(p(x)\\)\n\\(F(x)\\)\n\n\n\n\n\\(1\\)\n\\(0.2\\)\n\\(0.2\\)\n\n\n\\(2\\)\n\\(0.4\\)\n\\(0.6\\)\n\n\n\\(3\\)\n\\(0.3\\)\n\\(0.9\\)\n\n\n\\(4\\)\n\\(0.1\\)\n\\(1.0\\)\n\n\n\n\\[\n    F(x) = \\begin{cases}\n            0 & x &lt; 1 \\\\\n            0.2 & 1 \\leq x &lt; 2 \\\\\n            0.6 & 2 \\leq x &lt; 3 \\\\\n            0.9 & 3 \\leq x &lt; 4 \\\\\n            1 & x \\geq 4\n    \\end{cases}\n\\]\n\n\nCode\nlibrary(ggplot2)\n\n# Data\nx &lt;- c(1, 2, 3, 4)\nFx &lt;- c(0.2, 0.6, 0.9, 1.0)\n\n# Creating a data frame\ndata_cdf &lt;- data.frame(x, Fx)\n\n# CDF Plot\nggplot(data_cdf, aes(x = x, y = Fx)) +\n  geom_step(direction = \"hv\", linewidth=1) +  # Creates the step plot\n  geom_point(color=\"red\") +  # Optional: adds points at steps\n  ggtitle(\"Cumulative Distribution Function (CDF)\") +\n  xlab(\"x\") + ylab(\"F(x)\") +\n  theme_minimal()  # Optional: a cleaner theme for the plot"
  },
  {
    "objectID": "probStats/stat410.html",
    "href": "probStats/stat410.html",
    "title": "\nStatistics and Probability 2\n",
    "section": "",
    "text": "Statistics and Probability 2\n\nJan 16, 2024\n\n\nStats and Probability review\nDiscrete Random Variables 1\nDiscrete Random Variables 2"
  },
  {
    "objectID": "cs421/cs421.html",
    "href": "cs421/cs421.html",
    "title": "\nCS 421\n",
    "section": "",
    "text": "CS 421\n\n\n\nBasic OCaml Programming"
  },
  {
    "objectID": "CS374/lab7b.html",
    "href": "CS374/lab7b.html",
    "title": "\nDymamic programming\n",
    "section": "",
    "text": "Dymamic programming\nNov 22, 2023\nHow long is the longest increasing subsequence?\n4 1 2 7 6 5 8 9 3   1 2 6 8 9\nLets define a couple defintions to fromulate this:"
  },
  {
    "objectID": "CS374/lab7b.html#visulization-of-sequence",
    "href": "CS374/lab7b.html#visulization-of-sequence",
    "title": "\nDymamic programming\n",
    "section": "Visulization of sequence",
    "text": "Visulization of sequence\n\n\nCode\nlibrary(ggplot2)\n\n# Your original sequence\nsequence &lt;- c(4, 1, 2, 7, 6, 5, 8, 9, 3)\n\n# The known LIS\nlis &lt;- c(1, 2, 7, 8, 9)\n\n# Create a data frame for the original sequence\ndata &lt;- data.frame(x = 1:length(sequence), y = sequence)\n\n# Identify the indices of the LIS in the original sequence\nlis_indices &lt;- match(lis, sequence)\n\n# Create a data frame for the LIS\nlis_data &lt;- data.frame(x = lis_indices, y = lis)\n\n# Plotting the sequence and the LIS\nggplot(data, aes(x, y)) + \n  geom_point() + \n  geom_line(color = \"gray\") +\n  geom_point(data = lis_data, aes(x, y), color = \"red\") +\n  geom_line(data = lis_data, aes(x, y), color = \"red\") +\n  scale_x_continuous(breaks = 1:length(sequence)) +  \n  scale_y_continuous(breaks = 1:max(sequence)) +  \n  labs(x = \"Index\", y = \"Value\", title = \"Sequence with Highlighted LIS 1 2 7 8 9\")\n\n\n\n\n\n\n\n\nFigure 1: Plot of the sequence with the highlighted LIS 1 2 7 8 9\n\n\n\n\n\nThere could be multiple subsequences but we only care about the maximum increasing subsequence.\n\nHere is my animated plot:\n\n\n\nAnimated Plot\n\n\n\nDymamic programming\n\n\nDescribe and analyze dynamic programming algorithms for the following longest-subsequence problems.\n\nGiven an array \\(A[1...n]\\) of integers, compute the length of a longest increasing subsequence of \\(A\\)."
  },
  {
    "objectID": "CS374/dataStructures.html",
    "href": "CS374/dataStructures.html",
    "title": "\nData Strcutures\n",
    "section": "",
    "text": "Data Strcutures\n\nDec 31, 2023\n\n\nLinked List: notes\nHash Table: notes\nBinary Tress\nRandom Binary Search Trees\nRed-Black Trees\nHeaps\nSorting algorithms: notes\nGraph: notes\nExternal Memory Searching"
  },
  {
    "objectID": "CS374/lab1a.html",
    "href": "CS374/lab1a.html",
    "title": "\nString Induction\n",
    "section": "",
    "text": "String Induction\n\nNov 22, 2023"
  },
  {
    "objectID": "CS374/lab8a.html",
    "href": "CS374/lab8a.html",
    "title": "\nEdit distance\n",
    "section": "",
    "text": "Edit distance\n\n\n\nThe minimum number of character insertions, deletions, and substitutions needed to convert one string into another is known as the edit distance between those two strings.\nGiven two strings word1 and word2, return the minimum number of operations required to convert word1 to word2.\nYou have the following three operations permitted on a word:\n\nInsert a character\nDelete a character\nReplace a character"
  },
  {
    "objectID": "RL/mathematics.html",
    "href": "RL/mathematics.html",
    "title": "\nMathematics\n",
    "section": "",
    "text": "Mathematics\n\nJan 15, 2024\n\n\nLinear Algebra: notes\nProbability: notes"
  },
  {
    "objectID": "RL/gymnasium.html",
    "href": "RL/gymnasium.html",
    "title": "\nGymnasium\n",
    "section": "",
    "text": "Gymnasium\n\nJan 12, 2024\n\n\nEnv"
  },
  {
    "objectID": "RL/linearAlgebra.html",
    "href": "RL/linearAlgebra.html",
    "title": "\nLinear Algebra\n",
    "section": "",
    "text": "Linear Algebra\n\nJan 15, 2024\n\n Vector spaces\nA vector space \\(V\\) is a set, the elements of which are called \\(\\textbf{vectors}\\), on which two operations are defined: vectors can be added together, and vectors can be multiploed by real numbers (\\(\\textbf{scalars}\\)).\nWith this \\(V\\) must statisfy the following:\n\nThere exists an additive identity \\(\\mathbf{0}\\) in \\(V\\) such that \\(\\mathbf{x} + \\mathbf{0} = \\mathbf{x}\\) for all \\(\\mathbf{x} \\in V\\)\nFor each \\(\\mathbf{x} \\in V\\), there exists and additive inverse, \\(-\\mathbf{x}\\), such that \\(\\mathbf{x} + (-\\mathbf{x}) = \\mathbf{0}\\)\nThere exists a multiplicative identity (written 1) in \\(\\mathbb{R}\\) such that \\(1\\mathbf{x} = \\mathbf{x}\\) for all \\(\\mathbf{x} \\in V\\)\nCommutativity: \\(\\mathbf{x} + \\mathbf{y} = \\mathbf{y} + \\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y} \\in V\\)\nAssociativity: \\((\\mathbf{x} + \\mathbf{y}) + \\mathbf{z} = \\mathbf{x} + (\\mathbf{y} + \\mathbf{z})\\) and \\(\\alpha(\\beta \\mathbf{x}) = (\\alpha\\beta)\\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in V\\) and \\(\\alpha, \\beta \\in \\mathbb{R}\\)\nDistributivity: \\(\\alpha(\\mathbf{x} + \\mathbf{y}) = \\alpha\\mathbf{x} + \\alpha\\mathbf{y}\\) and \\((\\alpha + \\beta)\\mathbf{x} = \\alpha\\mathbf{x} + \\beta\\mathbf{x}\\) for all \\(\\mathbf{x}, \\mathbf{y} \\in V\\) and \\(\\alpha, \\beta \\in \\mathbb{R}\\)\n\nA set of vectors \\(\\mathbf{v}_1, \\cdots, \\mathbf{v}_n \\in V\\) is said to be \\(\\textbf{linearly independent}\\) if\n\\[\n    \\alpha \\mathbf{v}_n + \\cdots + \\alpha_n\\mathbf{v}_n = \\mathbf{0} \\quad \\text{ imlpies } \\quad\\alpha_1 = \\cdots = \\alpha_n = 0.\n\\]\nThe \\(\\textbf{span}\\) of \\(\\mathbf{v}_1, \\cdots, \\mathbf{v}_n \\in V\\) is the set of all vectors that can be expressed of a linear combination of them:\n\\[\n    \\text{span}\\{\\mathbf{v}_1, \\cdots, \\mathbf{v}_n\\} = \\{\\mathbf{v} \\in V : \\exists\\alpha, \\cdots, \\alpha_n \\text{ such that } \\alpha\\mathbf{v}_1 + \\cdots + \\alpha_n\\mathbf{v}_n = \\mathbf{v}\\}\n\\]\nIf the set of vectors is linearly independent and its span is the whole \\(V\\), those vectors are said to be a \\(\\textbf{basis}\\) for \\(V\\). In fact, every linear independent set of vectors forms a basis for its span.\nIf a vector space is spanned by a finite numbers of vectors, it is said to be \\(\\textbf{finite-dimensional}\\). Otherwise it is \\(\\textbf{infinite-dimensional}\\). The number of vectors in a basis for a finite-dimensional vector space \\(V\\) is called the \\(\\textbf{dimension}\\) of \\(V\\) and demoted dim \\(V\\).\n Euclidean space\nThe quintessential vector space is \\(\\textbf{Eculidean space}\\), which we denote \\(\\mathbb{R}^n\\). The vectors in this space consist of \\(n\\)-tuples of real numbers:\n\\[\n    \\mathbf{x} = \\left(x_1, x_2, \\cdots, x_n\\right)\n\\]\nIt is also useful to consider them as a \\(n \\times 1\\) matrices, or \\(\\textbf{column vectors}\\):\n\\[\n    \\mathbf{x} = \\begin{bmatrix}\n        x_{1} \\\\\n        x_{1} \\\\\n        \\vdots \\\\\n        x_{n}\n        \\end{bmatrix}\n\\]\nAddition and scalar multiplication are defined component-wise on vectors in \\(\\mathbb{R}^n\\). \\[\n    \\mathbf{x} + \\mathbf{y} = \\begin{bmatrix}\n        x_{1} + y{1} \\\\\n        x_{1} + y_{2}\\\\\n        \\vdots \\\\\n        x_{n} + y_{n}\n        \\end{bmatrix}, \\qquad \\alpha\\mathcal{x} = \\begin{bmatrix}\n                                                    \\alpha x_{1} \\\\\n                                                    \\vdots \\\\\n                                                    \\alpha x_{n}\n                                                    \\end{bmatrix}\n\\]\nEuclidean space is used to mathematically represent physical space, with notions such as distance, length, and angles. Although it becomes hard to visualize fot \\(n &gt; 3\\), these concepts generalize mathematically in obvious ways. Even when you’re wokring in more general settings than \\(\\mathbb{R}^n\\), it is often useful to visualize vector addition and scalalr multiplication in terms of \\(2D\\) vectors in the plane or \\(3D\\) vectors in space.\nSubspaces\nVector spaces can contain other vector spaces. If \\(V\\) is a vector space, then \\(\\mathcal{S} \\subseteq V\\) is said to be a \\(\\textbf{subspace}\\) of \\(V\\) if:\n\n\\(\\mathbf{0} \\in \\mathcal{S}\\)\n\\(\\mathcal{S}\\) is closed under addition: \\(\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}\\) implies \\(\\mathbf{x} + \\mathbf{y} \\in \\mathcal{S}\\)\n\\(\\mathcal{S}\\) is closed under scalar multiplication: \\(\\mathbf{x} \\in \\mathcal{S}, \\alpha \\in \\mathbb{R}\\) implies \\(\\alpha\\mathbf{x} \\in \\mathcal{S}\\)\n\nNote that \\(V\\) is always a subsapce of \\(V\\), as is the trivial vector space which contains only \\(\\mathbf{0}\\). As a concrete example, a line passing through the origin is a subsapce of Eculidean space. If \\(U\\) and \\(W\\) are subspaces of \\(V\\), then their sum is defined as\n\\[\n    U + W = \\{\\mathbf{u} + \\mathbf{w} \\mid \\mathbf{u} \\in U, \\mathbf{w} \\in W \\}\n\\]\nIf \\(U \\cap W = \\{\\mathbf{0}\\}\\), the sum is said to be a \\(\\textbf{direct sum}\\) and written \\(U \\oplus W\\). Every vector in \\(U \\oplus W\\) can be written uniquely as \\(\\mathbf{u} + \\mathbf{w}\\) for some \\(\\mathbf{u} \\in U\\) and \\(\\mathbf{w} \\in W\\).\nThe dimensions of sums of subspaces obey a friendly relationship:\n\\[\n    \\text{dim}(U + W) = \\text{dim } U + \\text{dim } W - \\text{dim}(U \\cap W)  \n\\]\nIt follows that\n\\[\n    \\text{dim}(U \\oplus W) = \\text{dim } U + \\text{dim } W\n\\]\nsince \\(\\text{dim}(U \\cap W) = \\text{dim}(\\{\\mathbf{0}\\}) = 0\\) if the sum is direct.\nLinear maps\nA \\(\\textbf{linear map}\\) is a function \\(T: V \\rightarrow W\\), where \\(V\\) and \\(W\\) are vector spaces, that statisfies\n\n\\(T(\\mathbf{x} + \\mathbf{y}) = T\\mathbf{x} + T\\mathbf{y} \\text{ for all } \\mathbf{x}, \\mathbf{y} \\in V\\)\n\\(T(\\alpha\\mathbf{x}) = \\alpha T\\mathbf{x} \\text{ for all } \\mathbf{x} \\in V, \\alpha \\in \\mathbb{R}\\)\n\nA linear map from \\(V\\) to itself is called a \\(\\textbf{linear operator}\\).\n Matrices and transposes\n\n\\(A\\) is a \\(m \\times n\\) real matrix, wrtitten \\(A \\in \\mathbb{R}^{m \\times n}\\), if: \\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\n\nwhere \\(a_{i, j} \\in \\mathbb{R}\\). The \\((i, j)\\)th entry of \\(A\\) is \\(A_{i, j} = a_{i, j}\\).\n\nThe transpose of \\(A \\in \\mathbb{R}\\) is define as: \\[\nA^{T} = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\n\\]\n\nsuch that, \\((A^{T})_{i, j} = A_{j, i}\\)\nNote: \\(x \\in \\mathbb{R}^{n}\\) is considered to be a columnn vector in \\(\\mathbb{R}^{n \\times 1}\\)\nSums and products of matrcies \n\nThe sum of matrices \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(B \\in \\mathbb{R}^{m \\times n}\\) is the matrix \\(A + B \\in \\mathbb{R}^{m \\times n}\\) such that \\[\n  (A + B)_{i, j} = A_{i, j} + B_{i, j}\n\\]\nThe product of matrices \\(A \\in \\mathbb{R}^{m \\times n}\\) and \\(B \\in \\mathbb{R}^{n \\times \\ell}\\) is the matrix \\(AB \\in \\mathbb{R}^{m \\times \\ell}\\) such that\n\n\\[\n    (AB)_{i, j} = \\sum_{k=1}^n   A_{i, k} B_{k, j}\n\\]"
  },
  {
    "objectID": "RL/intro.html",
    "href": "RL/intro.html",
    "title": "\nIntroduction\n",
    "section": "",
    "text": "Introduction\n\n\nIntroduction to MDPs and RL\nShortest Path\nFinding the shortest path:\nWhere we have a graph \\(G = (V, E)\\) where \\(V\\) are the vertices and \\(E\\) are the edges with weights \\(w\\). The vertices are the \\(\\textbf{states}\\) and the edges are the \\(\\textbf{actions}\\). Tne goal is you have a starting state and you wanna reach the end state while attating the least cost of the wieghts \\(w\\) along the edges. If the startegy is \\(\\textbf{Greedy}\\) then it is suboptimal due to dealyed affects of long term decisions. There must be a better way to long-term planning.\n\nBased on actions taken by the agent"
  },
  {
    "objectID": "CS421/cs421.html",
    "href": "CS421/cs421.html",
    "title": "\nCS 421\n",
    "section": "",
    "text": "CS 421\n\n\n\nPolymoprhic Typing Rules\nBasic OCaml Programming"
  },
  {
    "objectID": "CS411/cs411.html",
    "href": "CS411/cs411.html",
    "title": "\nCS 411\n",
    "section": "",
    "text": "CS 411\n\n\n\nDatabase Systems"
  },
  {
    "objectID": "RL/mdp.html",
    "href": "RL/mdp.html",
    "title": "\nMarkov decision process\n",
    "section": "",
    "text": "Markov decision process\n\\[\n\\def\\emph#1{\\textit{#1}}\n\\]\nIn reinforcement learning, the interactions between the agent and the environment are often described by a state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), transition function \\(P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})\\); where \\(\\Delta(\\mathcal{S})\\) is the space of probability distributions over \\(\\mathcal{S}\\) (i.e., the probability simplex). \\(P(s^\\prime \\mid s, a)\\) is the probability of transitioning into state \\(s^\\prime\\) upton taking action \\(a\\) in state \\(s\\). A reward function \\(R : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, R_{max}]\\). where \\(R_{max} &gt; 0\\) is a constant. \\(R(s, a)\\) is the immediate reward associated with taking action \\(a\\) in state \\(s\\). A discount factor \\(\\gamma \\in [0, 1)\\) , which defines the horizon for the problem."
  },
  {
    "objectID": "RL/mdp.html#opengym",
    "href": "RL/mdp.html#opengym",
    "title": "\nMarkov decision process\n",
    "section": "OpenGym",
    "text": "OpenGym\n\nInitialize the Environment\nimport gym \nenv = gym.make('MountainCar-v0')\n\n\nMountain car\nIn the Mountain Car Markov Decision Process (MDP), a car is randomly positioned at the lowest point of a sinusoidally-shaped valley. This MDP operates deterministically, providing a set of possible accelerative actions that can be executed to move the car either forward or backward. The objective is to judiciously use these accelerations to navigate the car to the target location, situated at the peak of the hill to the right. Within the gym framework, the mountain car scenario comes in two variants: one allowing for a discrete set of actions, and the other permitting a continuum of actions. The variant in question here is the one that employs discrete actions.\nclass MountainCarEnv(gym.Env):\n    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n        self.min_position = -1.2\n        self.max_position = 0.6\n        self.max_speed = 0.07\n        self.goal_position = 0.5\n        self.goal_velocity = goal_velocity\n\n        self.force = 0.001\n        self.gravity = 0.0025\n\n        self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n        self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n\n        self.render_mode = render_mode\n\n        self.screen_width = 600\n        self.screen_height = 400\n        self.screen = None\n        self.clock = None\n        self.isopen = True\n\n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)"
  },
  {
    "objectID": "RL/probability.html",
    "href": "RL/probability.html",
    "title": "\nProbability\n",
    "section": "",
    "text": "Probability\n\nJan 15, 2024\n\nProbability theory helps us deal with modeling with uncertainty.\nSuppose we perform a experiment like tossing a coin which has a fixed set of possible outcomes. This set is called the \\(\\textbf{sample sapce}\\) and we denote this space with \\(\\Omega\\).\nWe would like to define probabilities for some \\(\\textbf{events}\\), which are subsets of \\(\\Omega\\). The set of events is denoted \\(\\mathcal{F}\\). The \\(\\textbf{complement}\\) of the event \\(A\\) is another event, \\(A^{c} = \\Omega \\setminus A\\)\nThen we can define a \\(\\textbf{probability measure} \\:\\: \\mathbb{P}: \\mathcal{F} \\rightarrow [0, 1]\\) which must satisfy\n\n\\(\\mathbb{P}(\\Omega) = 1\\)\n\\(\\textbf{Countable addivity:}\\) for any countable collection of disjoint sets \\(\\{\\mathcal{A}_i\\} \\subseteq \\mathcal{F}\\),\n\n\\[\n    \\mathbb{P}\\left(\\bigcup_i A_i\\right) = \\sum_i \\mathbb{P}(A_i)\n\\]\nThe triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is called a \\(\\textbf{probability space}\\).\nIf \\(\\mathbb{P}(A) = 1\\), we say that \\(A\\) occurs \\(\\textbf{almost surely}\\), and conversely \\(A\\) occurs \\(\\textbf{almost never}\\) if \\(\\mathbb{P}(A) = 0.\\)\n\\(\\textbf{Proposition}\\): Let \\(A\\) be an event. Then\n\n\\(\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)\\)\nIf \\(B\\) is an event and \\(B \\subseteq A\\), then \\(\\mathbb{P}(B) \\leq \\mathbb{P}(A)\\).\n\\(0 = \\mathbb{P}(\\varnothing) \\leq \\mathbb{P}(A) \\leq \\mathbb{P}(\\Omega) = 1\\)\n\n\\(Proof\\):\nUsing the countable additivity of \\(\\mathbb{P}\\), we have\n\\[\n    \\mathbb{P}(A) + \\mathbb{P}(A^c) = \\mathbb{P}(A \\cup A^c) = \\mathbb{P}(\\Omega) = 1\n\\]\nTo show 2. suppose \\(B \\in \\mathcal{F}\\) and \\(B \\subseteq A\\). Then\n\\[\n    \\mathbb{P}(A) = \\mathbb{P}(B \\cup (A \\setminus B)) = \\mathbb{P}(B) + \\mathbb{P}(A \\setminus B) \\geq \\mathbb{P}(B)\n\\]\nas claimed.\nFor 3: the middle inequality follows from 2 since \\(\\varnothing \\subseteq A \\subseteq \\Omega\\). We also have:\n\\[\n    \\mathbb{P}(\\varnothing) = \\mathbb{P}(\\varnothing \\cup \\varnothing) = \\mathbb{P}(\\varnothing) + \\mathbb{P}(\\varnothing)\n\\]\nDiscrete random variables\n\nA random variable denoted as \\(\\text{r.v}\\) is a quantity that probabilistically takes on any of a possible range of values.\nA random variable \\(X\\) is descrete if it takes values in a countable set \\(\\mathcal{X} = \\{x_1, x_2, \\cdots,\\}.\\)\nMost random varibales have some certain distributioin for example Bernoulli, Binomial, Poisson, Geometric."
  },
  {
    "objectID": "CS421/rules.html",
    "href": "CS421/rules.html",
    "title": "\nPolymoprhic Typing Rule\n",
    "section": "",
    "text": "Polymoprhic Typing Rule\n\n\nWe will say a monomorphic type \\(\\tau\\) is an \\(\\textbf{instance}\\) of a polymorphic type \\(\\sigma\\) if there exists a monomorphic type \\(\\tau^\\prime\\), (a possibly empty) list of type variables \\(\\alpha_1, \\cdots, \\alpha_n\\), and a corresponding list of monomorphic types \\(\\tau_1, \\cdots, \\tau_n\\) such that \\(\\sigma = \\forall \\alpha_1, \\cdots, \\alpha_n\\). \\(\\tau^\\prime\\) and \\(\\tau = \\tau^\\prime\\left[\\tau_1 / \\alpha_1, \\cdots, \\tau_n / \\alpha_n\\right]\\), the type gotten by replacing each occurence of \\(\\alpha_i\\) in \\(\\tau^\\prime\\) by \\(\\tau_i\\). When using the rule below that require one type to be an instance of another, you should give the instantiation: \\(\\left[\\tau_1 / \\alpha_1, \\cdots, \\tau_n / \\alpha_n\\right]\\).\nIn simpler terms, think of a polymorphic type like a template and a monomorphic type like a filled-out form of that template. The process described is like saying, “To prove that this filled-out form is based on a specific template, show me how you substituted the placeholders in the template with actual information to get this form.\n\nSignatures\nPolymorphic constant signatures:\n\n\n\n\n\n\n\n\\(\\text{sig(true) = bool}\\)\n\\(\\text{sig(true) = bool}\\)\n\n\n\n\n\\(\\text{sig}(n) = \\text{int } n \\text{ an integer constant}\\)\n\\(\\text{sig}(f) = \\text{float } f \\text{ a floating point (real) constant}\\)\n\n\n\\(\\text{sig}(s) = \\text{string } s \\text{ a string constant}\\)\n\\(\\text{sig}([ \\:\\: ]) = \\forall \\alpha.\\alpha\\text{list}\\)\n\n\n\nPolymorphic Unary Primitive Operators:\n\n\n\n\n\n\n\n\\(\\text{sig(fst)} = \\forall\\alpha\\beta.(\\alpha * \\beta) \\rightarrow \\alpha\\)\n\\(\\text{sig(snd)} = \\forall\\alpha\\beta.(\\alpha * \\beta) \\rightarrow \\beta\\)\n\n\n\n\n\\(\\text{sig(hd)} = \\forall\\alpha.\\alpha\\text{list} \\rightarrow \\alpha\\)\n\\(\\text{sig(tl)} = \\forall\\alpha.\\alpha\\text{list} \\rightarrow \\alpha \\text{ list}\\)\n\n\n\\(\\text{sig}(\\sim) = \\text{int } \\rightarrow \\text{ int}\\)\n\\(\\text{sig(print\\_string)} = \\text{string } \\rightarrow \\text{ unit}\\)\n\n\n\\(\\text{sig(not)} = \\text{bool } \\rightarrow \\text{ bool}\\)\n\n\n\n\nPolymorphic Binary Primitive Operators:\n\n\n\n\n\n\n\n\\(\\text{sig}(\\oplus) = \\text{int} \\rightarrow \\text{ int} \\rightarrow \\text{ int for } \\oplus \\in \\{+, -, *, \\text{mod}, /\\}\\)\n\\(\\text{sig}( ^\\wedge ) = \\text{string } \\rightarrow \\text{ stirng} \\rightarrow \\text{ string}\\)\n\n\n\n\n\\(\\text{sig}(\\oplus) = \\text{float} \\rightarrow \\text{ float} \\rightarrow \\text{ float for } \\oplus \\in \\{+., -., *., /., **\\}\\)\n\\(\\text{sig}((\\_, \\_)) = \\forall\\alpha\\beta.\\alpha \\rightarrow \\beta \\rightarrow \\alpha * \\beta\\)\n\n\n\\(\\text{sig}(\\wr) = \\text{bool} \\rightarrow \\text{ bool} \\rightarrow \\text{ bool for } \\wr \\in \\{\\vert \\vert, \\&\\&\\}\\)\n\\(\\text{sig}(::) = \\forall\\alpha.\\alpha \\rightarrow \\alpha \\text{ list} \\rightarrow \\alpha \\text{ list}\\)\n\n\n\\(\\text{sig}(\\approx) = \\forall\\alpha.\\alpha \\rightarrow \\alpha \\rightarrow \\text{bool for } \\approx  \\:\\: \\in \\{&lt;, &gt;, =, &lt;=, &gt;=, &lt;&gt;\\}\\)\n\n\n\n\n\n\nRules\nConstants:\n\\(\\large{\\frac{}{\\Gamma \\:\\: \\vdash \\: c \\: : \\: \\tau }}\\) \\(\\text{\\large{CONST} } \\text{where }  c \\text{ is a constant listed above, and } \\tau \\text{ is an instance of sig}(c)\\)\nVariables:\n\\(\\large{\\frac{}{\\Gamma \\:\\: \\vdash \\: x \\: : \\: \\tau }}\\) \\(\\text{\\large{VAR}}\\) \\(\\text{ where } x \\text{ is a variable and } \\tau \\text{ is an instance of } \\Gamma(x)\\)"
  },
  {
    "objectID": "CS421/rules.html#signatures",
    "href": "CS421/rules.html#signatures",
    "title": "\nPolymoprhic Typing Rule\n",
    "section": "Signatures",
    "text": "Signatures\nPolymorphic constant signatures:\n\n\n\n\n\n\n\n\\(\\text{sig(true) = bool}\\)\n\\(\\text{sig(true) = bool}\\)\n\n\n\n\n\\(\\text{sig}(n) = \\text{int } n \\text{ an integer constant}\\)\n\\(\\text{sig}(f) = \\text{float } f \\text{ a floating point (real) constant}\\)\n\n\n\\(\\text{sig}(s) = \\text{string } s \\text{ a string constant}\\)\n\\(\\text{sig}([ \\:\\: ]) = \\forall \\alpha.\\alpha\\text{list}\\)\n\n\n\nPolymorphic Unary Primitive Operators:\n\n\n\n\n\n\n\n\\(\\text{sig(fst)} = \\forall\\alpha\\beta.(\\alpha * \\beta) \\rightarrow \\alpha\\)\n\\(\\text{sig(snd)} = \\forall\\alpha\\beta.(\\alpha * \\beta) \\rightarrow \\beta\\)\n\n\n\n\n\\(\\text{sig(hd)} = \\forall\\alpha.\\alpha\\text{list} \\rightarrow \\alpha\\)\n\\(\\text{sig(tl)} = \\forall\\alpha.\\alpha\\text{list} \\rightarrow \\alpha \\text{ list}\\)\n\n\n\\(\\text{sig}(\\sim) = \\text{int } \\rightarrow \\text{ int}\\)\n\\(\\text{sig(print\\_string)} = \\text{string } \\rightarrow \\text{ unit}\\)\n\n\n\\(\\text{sig(not)} = \\text{bool } \\rightarrow \\text{ bool}\\)\n\n\n\n\nPolymorphic Binary Primitive Operators:\n\n\n\n\n\n\n\n\\(\\text{sig}(\\oplus) = \\text{int} \\rightarrow \\text{ int} \\rightarrow \\text{ int for } \\oplus \\in \\{+, -, *, \\text{mod}, /\\}\\)\n\\(\\text{sig}( ^\\wedge ) = \\text{string } \\rightarrow \\text{ stirng} \\rightarrow \\text{ string}\\)\n\n\n\n\n\\(\\text{sig}(\\oplus) = \\text{float} \\rightarrow \\text{ float} \\rightarrow \\text{ float for } \\oplus \\in \\{+., -., *., /., **\\}\\)\n\\(\\text{sig}((\\_, \\_)) = \\forall\\alpha\\beta.\\alpha \\rightarrow \\beta \\rightarrow \\alpha * \\beta\\)\n\n\n\\(\\text{sig}(\\wr) = \\text{bool} \\rightarrow \\text{ bool} \\rightarrow \\text{ bool for } \\wr \\in \\{\\vert \\vert, \\&\\&\\}\\)\n\\(\\text{sig}(::) = \\forall\\alpha.\\alpha \\rightarrow \\alpha \\text{ list} \\rightarrow \\alpha \\text{ list}\\)\n\n\n\\(\\text{sig}(\\approx) = \\forall\\alpha.\\alpha \\rightarrow \\alpha \\rightarrow \\text{bool for } \\approx  \\:\\: \\in \\{&lt;, &gt;, =, &lt;=, &gt;=, &lt;&gt;\\}\\)"
  },
  {
    "objectID": "CS421/rules.html#rules",
    "href": "CS421/rules.html#rules",
    "title": "\nPolymoprhic Typing Rule\n",
    "section": "Rules",
    "text": "Rules"
  },
  {
    "objectID": "probStats/discreteVariable.html",
    "href": "probStats/discreteVariable.html",
    "title": "\nDiscrete Random Variables 1\n",
    "section": "",
    "text": "Discrete Random Variables 1\n\nJan 17, 2024\n\nTerminology\nInformally, a \\(\\textbf{random variable}\\) is a quantity \\(X\\) whose value depends on some random event. The \\(\\textbf{space (or range)}\\) of \\(X\\) is the set \\(S\\) of possible values of \\(X\\). If this set \\(S\\) is finite or countable (i.e., can be listed as a sequence \\((x_1. x_2, \\cdots\\)), the random variable is called \\(\\textbf{discrete}\\).\nGeneral formulas\n\n\\(\\textbf{Probability mass function (p.m.f):}\\)\n\n\\(f(x) = \\mathbb{P}(X = x)\\) for all \\(x \\in S\\)\n\\(f(x) \\geq 0\\) and \\(\\underset{x \\in S}{\\sum} f(x) = 1\\)\n\\(\\textbf{Uniform distribution}\\) on a set \\(S\\): Each of the values \\(x \\in S\\) has the same probability, i.e., \\(f(x) = 1/n\\) for each value \\(x\\), where \\(n\\) is the number of values.\n\n\\(\\textbf{Expectation (mean):}\\)\n\n\\(\\mu = \\mathbb{E}(X) = \\underset{x \\in S}{\\sum} x \\cdot f(x)\\)\n\\(\\mathbb{E}(c) = c, \\:\\: \\mathbb{E}(cX) = c\\mathbb{E}(X), \\:\\: \\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\\)\n\\(\\textbf{Expectation of a function of } X: \\mathbb{E}(u(X)) = \\underset{x \\in S}{\\sum} u(x)f(x)\\)\n\n\\(\\textbf{Variance:}\\)\n\n\\(\\sigma^2 = \\text{Var}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\\)\n\\(\\text{Var}(X) = \\mathbb{E}((X - \\mu)^2)\\)\n\\(\\text{Var}(c) = 0, \\:\\: \\text{Var}(cX) = c^2\\:\\text{Var}(X), \\:\\: \\text{Var}(X + c) = \\text{Var}(X)\\)\n\\(\\textbf{Standard deviation}: \\sigma = \\sqrt{\\text{Var}(X)}\\)\n\n\\(\\textbf{Moment-generating function:}\\)\n\n\\(M(t) = \\mathbb{E}(\\mathcal{e}^{tX}) = \\underset{x \\in S}{\\sum} \\mathcal{e}^{tx} f(x)\\)\nThe derivatives of \\(M(t)\\) at 0 generate the moments of \\(X: M^{\\prime} (0) = \\mathbb{E}(X), M^{\\prime\\prime}(0) = \\mathbb{E}(X^2), \\:\\: M^{\\prime\\prime\\prime} (0) = \\mathbb{E}(X^3),\\) etc."
  },
  {
    "objectID": "probStats/discreteVariable.html#terminology",
    "href": "probStats/discreteVariable.html#terminology",
    "title": "\nDiscrete Random Variables 1\n",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "probStats/discreteVariable2.html",
    "href": "probStats/discreteVariable2.html",
    "title": "\nDiscrete Random Variables 2\n",
    "section": "",
    "text": "Discrete Random Variables 2\n\nJan 17, 2024\n\nThe Big Three\nThe following is a list of essentail formulaas for the three most important discrete distributions: \\(\\textbf{binomial, geometric, and Poisson.}\\)\n\n\\(\\textbf{Binomial distribution } b(n, p):\\)\n\n\\(n \\text{ (positive integer), } p \\: (0 \\leq p \\leq 1)\\)\n\\(\\textbf{p.m.f: } f(x) = {n \\choose x}p^x (1 - p)^{n - x} \\:\\: (x = 0, 1, 2, ..., n)\\)\n\\(\\textbf{Expectation and variance: } \\mu = np, \\:\\: \\sigma^2 = np(1 - p)\\)\n\\(\\textbf{Arises as: }\\) Distribution of number of successes in success/failure trials (Bernoulli trials)\n\n\\(\\textbf{Geometric distribution:}\\)\n\n\\(p \\: (0 &lt; p &lt; 1)\\)\n\\(\\textbf{p.m.f: } f(x) = (1 - p)^{x - 1} p \\:\\: (x =1, 2, ...)\\)\n\\(\\textbf{Expectation and variance: } \\mu = 1/p, \\: \\sigma^2 = (1 - p)/p^2\\)\n\\(\\textbf{Geometric series: } \\overset{\\infty}{\\underset{n=0}{\\sum}} r^n = \\frac{1}{1 - r} (| r | &lt; 1)\\)\n\\(\\textbf{Arises as: }\\) Distribution of trial at which the first success occurs in success/failure trial sequence\n\n\\(\\textbf{Poisson disctribution:}\\)\n\n\\(\\lambda &gt; 0\\)\n\\(\\textbf{p.m.f: } f(x) = \\mathcal{e}^{-\\lambda \\frac{\\lambda^x}{x!}} \\:\\: (x = 0, 1, 2, ...)\\)\n\\(\\textbf{Expectation and variance: }\\overset{\\infty}{\\underset{n=0}{\\sum}} \\frac{\\lambda^n}{n!} = \\mathcal{e}^\\lambda\\)\n\\(\\textbf{Arises as: }\\) Distribution of number of occurrences of rare events, such as accidents, insurance claims, etc.\n\n\nOther discrete distributions\n\n\\(\\textbf{Hypergeometric distribution:} f(x) = \\frac{ {N_1 \\choose x} {N_2 \\choose n - x} }{N \\choose n}\\), \\(x = 0, 1, ..., N_1, n - x \\leq N_2\\)\n\\(\\textbf{Negative binomial distribution:} f(x) = {x - 1 \\choose r - 1}(1 - p)^{x - r}p^r\\), \\(\\quad x = r, r + 1, ...\\)\n\nBinomial coefficients\n\n\\(\\textbf{Definition: } \\text{For } n = 1, 2, ... \\text{ and } k = 0, 1, ..., n, {n \\choose k} = \\frac{n!}{k!(n - k)!}\\)\n\\(\\textbf{Alternate notations: } {}_n C_k \\text{ or } C(n, k)\\)\n\\(\\text{Other definition:} {n \\choose k} = \\frac{n(n-1)...(n - k + 1)}{k!}\\)\n\\(\\textbf{Symmetry property: } {n \\choose k} = {n \\choose n - k}\\)\n\\(\\textbf{Special cases: } {n \\choose 0} = {n \\choose n} = 1, \\:\\: {n \\choose 1} = {n \\choose n - 1} = n\\)\n\\(\\textbf{Binomial Theorem: } (x + y)^n = \\overset{n}{\\underset{k=0}{\\sum}} {n \\choose k} x^k y^{n - k}\\)\n\\(\\textbf{Binomial Theorem, special case: } (x + y)^n = \\overset{n}{\\underset{k=0}{\\sum}} {n \\choose k} p^k(1 - p)^{n - k} = 1\\)\n\\(\\textbf{Combinatorial Interpretations: } {n \\choose k} \\text{ represents }\\)\n\n\nthe number of ways to select \\(k\\) objects out of \\(n\\) given objects (in the sense of unordered samples wihtout replacement)\nthe number of \\(k\\)-element subsets of an \\(n\\)-element set\nthe number of \\(n\\)-letter \\(\\text{HT}\\) sequences with exactly \\(k\\) \\(\\text{H's}\\) and \\(n - k\\) \\(\\text{T's}\\)\n\n\n\\(\\textbf{Binomial distribution: }\\) Given a positive integer \\(n\\) and a number \\(p\\) with \\(0 &lt; p &lt; 1\\), the binomial distribution \\(b(n, p)\\) is the distribution with density (p.m.f) \\(f(x) = {n \\choose x} p^x(1 - p)^{n - x}\\), for \\(x = 0, 1, ..., n\\)."
  }
]